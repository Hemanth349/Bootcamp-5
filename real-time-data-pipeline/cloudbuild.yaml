serviceAccount: cloud-build-sa@ancient-cortex-465315-t4.iam.gserviceaccount.com

options:
  logging: CLOUD_LOGGING_ONLY

steps:
  # Clean up any previous Terraform dirs (optional but safe)
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'sh'
    args: ['-c', 'rm -rf .terraform']
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Init
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform init \
          -backend-config="bucket=your-tf-state-bucket1" \
          -backend-config="prefix=real-time-data-pipeline/terraform"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Import existing resources to avoid conflicts
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform import google_pubsub_topic.stream_topic stream-topic || echo "Pub/Sub topic already imported"
        terraform import google_storage_bucket.raw_data_bucket ancient-cortex-465315-t4-raw-data || echo "Bucket already imported"
        terraform import google_bigquery_dataset.processed_dataset streaming_output || echo "Dataset already imported"
        terraform import google_bigquery_table.processed_table projects/ancient-cortex-465315-t4/datasets/streaming_output/tables/user_actions || echo "Table already imported"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Apply to create/update resources
  - name: 'hashicorp/terraform:light'
    entrypoint: 'terraform'
    args: ['apply', '-auto-approve']
    dir: 'real-time-data-pipeline/terraform'

 # Step 3: Create Publisher VM only if not exists
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'create-vm'
    entrypoint: 'bash'
    args:
      - -c
      - |
        if gcloud compute instances describe pubsub-publisher-vm --zone=us-central1-a &> /dev/null; then
          echo "VM already exists, skipping creation"
        else
          gcloud compute instances create pubsub-publisher-vm \
            --zone=us-central1-a \
            --machine-type=e2-micro \
            --image-family=debian-11 \
            --image-project=debian-cloud || exit 1
        fi

  # Step 4: Wait for VM to be RUNNING
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'wait-for-vm'
    entrypoint: 'bash'
    args:
      - -c
      - |
        echo "Waiting for pubsub-publisher-vm to be RUNNING..."
        for i in {1..30}; do
          status=$(gcloud compute instances describe pubsub-publisher-vm --zone=us-central1-a --format='get(status)')
          if [ "$status" == "RUNNING" ]; then
            echo "VM is RUNNING"
            exit 0
          fi
          echo "Current status: $status; waiting 5 seconds..."
          sleep 5
        done
        echo "Timed out waiting for VM to be RUNNING"
        exit 1

  # Step 5: Copy publisher.py to the VM
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'copy-script'
    args:
      - compute
      - scp
      - real-time-data-pipeline/pubsub/publisher.py
      - pubsub-publisher-vm:/opt/app/publisher.py
      - --zone=us-central1-a

  # Step 6: Run publisher.py in background
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'start-script'
    entrypoint: 'bash'
    args:
      - -c
      - |
        gcloud compute ssh pubsub-publisher-vm \
          --zone=us-central1-a \
          --command="nohup python3 /opt/app/publisher.py > /opt/app/publisher.log 2>&1 &"
