serviceAccount: cloud-build-sa@ancient-cortex-465315-t4.iam.gserviceaccount.com

options:
  logging: CLOUD_LOGGING_ONLY

steps:
  # Clean up any previous Terraform dirs (optional but safe)
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'sh'
    args: ['-c', 'rm -rf .terraform']
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Init
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform init \
          -backend-config="bucket=your-tf-state-bucket1" \
          -backend-config="prefix=real-time-data-pipeline/terraform"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Import existing resources to avoid conflicts
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform import google_pubsub_topic.stream_topic stream-topic || echo "Pub/Sub topic already imported"
        terraform import google_storage_bucket.raw_data_bucket ancient-cortex-465315-t4-raw-data || echo "Bucket already imported"
        terraform import google_bigquery_dataset.processed_dataset streaming_output || echo "Dataset already imported"
        terraform import google_bigquery_table.processed_table projects/ancient-cortex-465315-t4/datasets/streaming_output/tables/user_actions || echo "Table already imported"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Apply to create/update resources
  - name: 'hashicorp/terraform:light'
    entrypoint: 'terraform'
    args: ['apply', '-auto-approve']
    dir: 'real-time-data-pipeline/terraform'

steps:
  # Terraform init
  - name: 'hashicorp/terraform:light'
    entrypoint: sh
    args:
      - -c
      - |
        terraform init \
          -backend-config="bucket=your-tf-state-bucket1" \
          -backend-config="prefix=real-time-data-pipeline/terraform"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform apply
  - name: 'hashicorp/terraform:light'
    entrypoint: terraform
    args: ["apply", "-auto-approve"]
    dir: 'real-time-data-pipeline/terraform'

  # Create VM (ignore error if exists)
  - name: 'gcr.io/cloud-builders/gcloud'
    id: create-vm
    entrypoint: sh
    args:
      - -c
      - |
        gcloud compute instances create pubsub-publisher-vm \
          --zone=us-central1-a \
          --machine-type=e2-micro \
          --image-family=debian-11 \
          --image-project=debian-cloud \
          --quiet || echo "VM likely already exists"

  # Copy script to VM
  - name: 'gcr.io/cloud-builders/gcloud'
    id: copy-script
    args:
      - compute
      - scp
      - real-time-data-pipeline/pubsub/publisher.py
      - pubsub-publisher-vm:/opt/app/publisher.py
      - --zone=us-central1-a
      - --quiet

  # Run publisher.py on VM
  - name: 'gcr.io/cloud-builders/gcloud'
    id: run-script
    entrypoint: sh
    args:
      - -c
      - |
        gcloud compute ssh pubsub-publisher-vm --zone=us-central1-a --command="nohup python3 /opt/app/publisher.py > /opt/app/publisher.log 2>&1 &"

