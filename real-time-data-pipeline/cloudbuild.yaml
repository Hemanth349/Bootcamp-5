serviceAccount: cloud-build-sa@ancient-cortex-465315-t4.iam.gserviceaccount.com

options:
  logging: CLOUD_LOGGING_ONLY

steps:
  # Clean up any previous Terraform dirs (optional but safe)
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'sh'
    args: ['-c', 'rm -rf .terraform']
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Init
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform init \
          -backend-config="bucket=your-tf-state-bucket1" \
          -backend-config="prefix=real-time-data-pipeline/terraform"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Import existing resources to avoid conflicts
  - name: 'hashicorp/terraform:light'
    entrypoint: 'sh'
    args:
      - -c
      - |
        terraform import google_pubsub_topic.stream_topic stream-topic || echo "Pub/Sub topic already imported"
        terraform import google_storage_bucket.raw_data_bucket ancient-cortex-465315-t4-raw-data || echo "Bucket already imported"
        terraform import google_bigquery_dataset.processed_dataset streaming_output || echo "Dataset already imported"
        terraform import google_bigquery_table.processed_table projects/ancient-cortex-465315-t4/datasets/streaming_output/tables/user_actions || echo "Table already imported"
    dir: 'real-time-data-pipeline/terraform'

  # Terraform Apply to create/update resources
  - name: 'hashicorp/terraform:light'
    entrypoint: 'terraform'
    args: ['apply', '-auto-approve']
    dir: 'real-time-data-pipeline/terraform'

  # Create VM (ignore error if exists)
  - name: 'gcr.io/cloud-builders/gcloud'
    id: create-vm
    entrypoint: sh
    args:
      - -c
      - |
        gcloud compute instances create pubsub-publisher-vm \
          --zone=us-central1-a \
          --machine-type=e2-micro \
          --image-family=debian-11 \
          --image-project=debian-cloud \
          --scopes=https://www.googleapis.com/auth/cloud-platform \
          --tags=allow-ssh \
          --metadata=ssh-keys="sriyakalamata64:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHawoZTGDDoVASFyV4hKmrz+KAZ4Te0PTtZsXK6iPp+7Bov9gJA5rHFkdFvHI50mIWKg3UYfYoARrIDRfW8uw8vPZYRf+nYhtvLu25KQvVBg0vD7fZDv/1deFVtoAUgzvREKiuILNP3n3CLTAxb7Grgo5+kIW8bqWLcTpuWEeFrV5fvRKBgXlSDXFlEc9dwQ3VxtOtdLeIJokGlhN4q74pGW1ViEz+0u7sd/KE0xEvNoj7TybvU78fjcqANgA5AD/5lZG92VK7hy5hkEM77igpPC42QsjRTtcUjBE037JSZsUM3eb0C6D8G+Vf+kjx2x3o+p9AtKDAcTnv+V7KO2zx sriyakalamata64@pubsub-publisher-vm" \
          --quiet || echo "VM likely already exists"
        echo "Waiting 30 seconds for SSH keys to propagate..."
        sleep 30

  # Copy script to VM by SSH and gsutil copy from bucket
  - name: 'gcr.io/cloud-builders/gcloud'
    id: copy-from-bucket
    entrypoint: sh
    args:
      - -c
      - |
        gcloud compute ssh pubsub-publisher-vm --zone=us-central1-a --command="mkdir -p /opt/app && gsutil cp gs://ancient-cortex-465315-t4-raw-data/publisher.py /opt/app/publisher.py"

  # Run publisher.py on VM
  - name: 'gcr.io/cloud-builders/gcloud'
    id: run-script
    entrypoint: sh
    args:
      - -c
      - |
        gcloud compute ssh pubsub-publisher-vm --zone=us-central1-a --command="nohup python3 /opt/app/publisher.py > /opt/app/publisher.log 2>&1 &"
